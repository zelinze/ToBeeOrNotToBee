{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:09:11.129290Z",
     "start_time": "2024-05-29T22:07:55.640302Z"
    }
   },
   "source": [
    "import cv2 \n",
    "from PIL import Image \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skimage.color import rgb2gray\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "img_dir = 'train'\n",
    "mask_dir = 'train/masks'\n",
    "excel_file = 'train/classif.xlsx'\n",
    "data_feature_file = 'data_features.csv'\n",
    "\n",
    "# Load images\n",
    "def load_images(img_dir, count):\n",
    "    images = []\n",
    "    for i in range(1, count + 1):\n",
    "        img_path = os.path.join(img_dir, f\"{i}.jpg\")\n",
    "        if os.path.exists(img_path):\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB format\n",
    "            images.append(img)\n",
    "        else:\n",
    "            print(f\"Image {img_path} not found.\")\n",
    "    return images\n",
    "\n",
    "# Load masks\n",
    "def load_masks(mask_dir, count):\n",
    "    masks = []\n",
    "    for i in range(1, count + 1):\n",
    "        mask_path = os.path.join(mask_dir, f\"binary_{i}.tif\")\n",
    "        if os.path.exists(mask_path):\n",
    "            mask = Image.open(mask_path)\n",
    "            mask = np.array(mask)\n",
    "            # 检查掩码是否为灰度图像\n",
    "            if mask.ndim == 3:\n",
    "                mask = rgb2gray(mask)\n",
    "            masks.append(mask)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Mask {mask_path} not found.\")\n",
    "    return masks\n",
    "\n",
    "# Load classification file\n",
    "def load_classification(excel_file):\n",
    "    if os.path.exists(excel_file):\n",
    "        return pd.read_excel(excel_file)\n",
    "    else:\n",
    "        print(f\"Excel file {excel_file} not found.\")\n",
    "        return None\n",
    "    \n",
    "# Load features\n",
    "def load_features(data_feature_file):\n",
    "    if os.path.exists(data_feature_file):\n",
    "        return pd.read_csv(data_feature_file)\n",
    "    else:\n",
    "        print(f\"Feature file {data_feature_file} not found.\")\n",
    "        return None    \n",
    "    \n",
    "images = load_images(img_dir, 250)\n",
    "masks = load_masks(mask_dir, 250)\n",
    "classif_df = load_classification(excel_file)   \n",
    "features_df = load_features(data_feature_file)  \n",
    "\n",
    "# 处理标签\n",
    "def process_labels(df):\n",
    "    bug_types = []\n",
    "    species = []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        bugs = row['bug_type']\n",
    "        species_info = row['species']\n",
    "        \n",
    "        # 处理x2\n",
    "        if ' x2' in species_info:\n",
    "            bugs = [bugs.replace(' x2','')] * 2\n",
    "            species_info = [species_info.replace(' x2','')] * 2\n",
    "        else:\n",
    "            bugs = bugs.split(' & ')\n",
    "            species_info = species_info.split(' & ')\n",
    "        \n",
    "        # 处理问号\n",
    "        bugs = [bug.replace(' ?', '') for bug in bugs]\n",
    "        species_info = [specie.replace(' ?', '') for specie in species_info]\n",
    "        \n",
    "        bug_types.append(bugs)\n",
    "        species.append(species_info)\n",
    "    \n",
    "    df['bug_type'] = bug_types\n",
    "    df['species'] = species\n",
    "    return df\n",
    "\n",
    "classif_df = process_labels(load_classification(excel_file))\n",
    "# 使用MultiLabelBinarizer转换标签\n",
    "mlb_bug_type = MultiLabelBinarizer()\n",
    "mlb_species = MultiLabelBinarizer()\n",
    "bug_type_encoded = mlb_bug_type.fit_transform(classif_df['bug_type'])\n",
    "species_encoded = mlb_species.fit_transform(classif_df['species'])\n",
    "\n",
    "# 转换为DataFrame\n",
    "bug_type_df = pd.DataFrame(bug_type_encoded, columns=mlb_bug_type.classes_)\n",
    "species_df = pd.DataFrame(species_encoded, columns=mlb_species.classes_)\n",
    "#以及其他数据集\n",
    "images = load_images(img_dir, 250)\n",
    "masks = load_masks(mask_dir, 250)\n",
    "features_df = load_features(data_feature_file)  \n",
    "\n",
    "\n",
    "\n",
    "# 合并所有数据\n",
    "all_data_df = pd.concat([classif_df.drop(columns=['bug_type', 'species']), bug_type_df, species_df, features_df], axis=1)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask 1 shape: (4000, 6000)\n",
      "Mask 2 shape: (4000, 6000)\n",
      "Mask 3 shape: (4000, 6000)\n",
      "Mask 4 shape: (4000, 6000)\n",
      "Mask 5 shape: (4000, 6000)\n",
      "Mask 6 shape: (4000, 6000)\n",
      "Mask 7 shape: (4000, 6000)\n",
      "Mask 8 shape: (4000, 6000)\n",
      "Mask 9 shape: (4000, 6000)\n",
      "Mask 10 shape: (4000, 6000)\n",
      "Mask 11 shape: (4000, 6000)\n",
      "Mask 12 shape: (4000, 6000)\n",
      "Mask 13 shape: (4000, 6000)\n",
      "Mask 14 shape: (4000, 6000)\n",
      "Mask 15 shape: (4000, 6000)\n",
      "Mask 16 shape: (6000, 4000)\n",
      "Mask 17 shape: (4000, 6000)\n",
      "Mask 18 shape: (4000, 6000)\n",
      "Mask 19 shape: (4000, 6000)\n",
      "Mask 20 shape: (4000, 6000)\n",
      "Mask 21 shape: (4000, 6000)\n",
      "Mask 22 shape: (4000, 6000)\n",
      "Mask 23 shape: (4000, 6000)\n",
      "Mask 24 shape: (4000, 6000)\n",
      "Mask 25 shape: (4000, 6000)\n",
      "Mask 26 shape: (4000, 6000)\n",
      "Mask 27 shape: (4000, 6000)\n",
      "Mask 28 shape: (4000, 6000)\n",
      "Mask 29 shape: (4000, 6000)\n",
      "Mask 30 shape: (4000, 6000)\n",
      "Mask 31 shape: (4000, 6000)\n",
      "Mask 32 shape: (4000, 6000)\n",
      "Mask 33 shape: (4000, 6000)\n",
      "Mask 34 shape: (4000, 6000)\n",
      "Mask 35 shape: (4000, 6000)\n",
      "Mask 36 shape: (4000, 6000)\n",
      "Mask 37 shape: (4000, 6000)\n",
      "Mask 38 shape: (4000, 6000)\n",
      "Mask 39 shape: (4000, 6000)\n",
      "Mask 40 shape: (4000, 6000)\n",
      "Mask 41 shape: (4000, 6000)\n",
      "Mask 42 shape: (4000, 6000)\n",
      "Mask 43 shape: (4000, 6000)\n",
      "Mask 44 shape: (4000, 6000)\n",
      "Mask 45 shape: (4000, 6000)\n",
      "Mask 46 shape: (4000, 6000)\n",
      "Mask 47 shape: (4000, 6000)\n",
      "Mask 48 shape: (4000, 6000)\n",
      "Mask 49 shape: (4000, 6000)\n",
      "Mask 50 shape: (4000, 6000)\n",
      "Mask 51 shape: (4000, 6000)\n",
      "Mask 52 shape: (4000, 6000)\n",
      "Mask 53 shape: (4000, 6000)\n",
      "Mask 54 shape: (4000, 6000)\n",
      "Mask 55 shape: (4000, 6000)\n",
      "Mask 56 shape: (4000, 6000)\n",
      "Mask 57 shape: (4000, 6000)\n",
      "Mask 58 shape: (4000, 6000)\n",
      "Mask 59 shape: (4000, 6000)\n",
      "Mask 60 shape: (4000, 6000)\n",
      "Mask 61 shape: (4000, 6000)\n",
      "Mask 62 shape: (4000, 6000)\n",
      "Mask 63 shape: (4000, 6000)\n",
      "Mask 64 shape: (4000, 6000)\n",
      "Mask 65 shape: (4000, 6000)\n",
      "Mask 66 shape: (4000, 6000)\n",
      "Mask 67 shape: (4000, 6000)\n",
      "Mask 68 shape: (4000, 6000)\n",
      "Mask 69 shape: (4000, 6000)\n",
      "Mask 70 shape: (4000, 6000)\n",
      "Mask 71 shape: (4000, 6000)\n",
      "Mask 72 shape: (4000, 6000)\n",
      "Mask 73 shape: (4000, 6000)\n",
      "Mask 74 shape: (4000, 6000)\n",
      "Mask 75 shape: (4000, 6000)\n",
      "Mask 76 shape: (4000, 6000)\n",
      "Mask 77 shape: (4000, 6000)\n",
      "Mask 78 shape: (4000, 6000)\n",
      "Mask 79 shape: (4000, 6000)\n",
      "Mask 80 shape: (4000, 6000)\n",
      "Mask 81 shape: (4000, 6000)\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 22.9 MiB for an array with shape (4000, 6000) and data type uint8",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 72\u001B[0m\n\u001B[0;32m     69\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m    \n\u001B[0;32m     71\u001B[0m images \u001B[38;5;241m=\u001B[39m load_images(img_dir, \u001B[38;5;241m250\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m masks \u001B[38;5;241m=\u001B[39m \u001B[43mload_masks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m250\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m     73\u001B[0m classif_df \u001B[38;5;241m=\u001B[39m load_classification(excel_file)   \n\u001B[0;32m     74\u001B[0m features_df \u001B[38;5;241m=\u001B[39m load_features(data_feature_file)  \n",
      "Cell \u001B[1;32mIn[3], line 44\u001B[0m, in \u001B[0;36mload_masks\u001B[1;34m(mask_dir, count)\u001B[0m\n\u001B[0;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(mask_path):\n\u001B[0;32m     43\u001B[0m     mask \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mopen(mask_path)\n\u001B[1;32m---> 44\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     45\u001B[0m     \u001B[38;5;66;03m# 检查掩码是否为灰度图像\u001B[39;00m\n\u001B[0;32m     46\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m3\u001B[39m:\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 22.9 MiB for an array with shape (4000, 6000) and data type uint8"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "随机森林和支持向量机部分"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:18.965315Z",
     "start_time": "2024-05-29T19:55:18.840249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "    # Prepare feature matrix and target vector\n",
    "X = all_data_df.drop(columns=['ID']).values\n",
    "y = bug_type_encoded[:,0]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 49)\n",
      "(200,)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:20.868549Z",
     "start_time": "2024-05-29T19:55:18.969586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(RandomForestClassifier(n_estimators=100), X_train, y_train, cv=5)\n",
    "scores"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.   , 1.   , 0.975, 1.   , 1.   ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:22.862596Z",
     "start_time": "2024-05-29T19:55:20.875852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, max_depth=3)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(f\"Random Forest Accuracy: {rf_accuracy}\")\n",
    "\n",
    "# SVM\n",
    "svm_model = SVC(kernel='linear', probability=True, C=0.1, random_state=42)\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_pred = svm_model.predict(X_test)\n",
    "svm_accuracy = accuracy_score(y_test, svm_pred)\n",
    "print(f\"SVM Accuracy: {svm_accuracy}\")\n",
    "\n",
    "# Evaluate models on training set\n",
    "rf_train_pred = rf_model.predict(X_train)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_pred)\n",
    "print(f\"Random Forest Training Accuracy: {rf_train_accuracy}\")\n",
    "\n",
    "svm_train_pred = svm_model.predict(X_train)\n",
    "svm_train_accuracy = accuracy_score(y_train, svm_train_pred)\n",
    "print(f\"SVM Training Accuracy: {svm_train_accuracy}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 1.0\n",
      "SVM Accuracy: 1.0\n",
      "Random Forest Training Accuracy: 1.0\n",
      "SVM Training Accuracy: 1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "随机森林和支持向量机2.0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T20:54:19.948223Z",
     "start_time": "2024-05-29T20:54:18.034878Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 合并特征和标签数据\n",
    "labels_df = bug_type_df  # 或者使用 species_df 作为标签\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df, labels_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 确保数据集大小一致\n",
    "print(f\"训练集大小: {X_train.shape[0]}, 测试集大小: {X_test.shape[0]}\")\n",
    "\n",
    "# 训练随机森林分类器\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测并计算准确度\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"随机森林分类准确度: {accuracy_rf}\")\n",
    "\n",
    "\n",
    "# 使用OneVsRest策略做svm\n",
    "svm_classifier = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测并计算准确度\n",
    "y_pred_svm = svm_classifier.predict(X_test)\n",
    "\n",
    "# 计算准确度\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"支持向量机分类准确度 (One-vs-Rest): {accuracy_svm}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集大小: 200, 测试集大小: 50\n",
      "随机森林分类准确度: 0.5\n",
      "支持向量机分类准确度 (One-vs-Rest): 0.42\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T22:00:55.457597Z",
     "start_time": "2024-05-29T22:00:54.056589Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "def extract_sift_features(images, masks):\n",
    "    sift = cv2.SIFT_create()\n",
    "    all_features = []\n",
    "\n",
    "    for img, mask in zip(images, masks):\n",
    "        keypoints, descriptors = sift.detectAndCompute(img, mask)\n",
    "        if descriptors is not None:\n",
    "            all_features.append(descriptors)\n",
    "        else:\n",
    "            print(\"No descriptors found for an image.\")\n",
    "    \n",
    "    return all_features\n",
    "\n",
    "sift_features = extract_sift_features(images, masks)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 聚类数目（可调）\n",
    "n_clusters = 100\n",
    "\n",
    "# 将所有特征拼接成一个矩阵\n",
    "all_descriptors = np.vstack(sift_features)\n",
    "\n",
    "# 使用KMeans进行聚类\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "kmeans.fit(all_descriptors)\n",
    "\n",
    "# 创建Bag of Words表示\n",
    "def create_bow_histogram(sift_features, kmeans):\n",
    "    histograms = []\n",
    "    for descriptors in sift_features:\n",
    "        histogram = np.zeros(kmeans.n_clusters)\n",
    "        if descriptors is not None:\n",
    "            cluster_assignments = kmeans.predict(descriptors)\n",
    "            for assignment in cluster_assignments:\n",
    "                histogram[assignment] += 1\n",
    "        histograms.append(histogram)\n",
    "    return histograms\n",
    "\n",
    "bow_histograms = create_bow_histogram(sift_features, kmeans)\n",
    "bow_histograms = np.array(bow_histograms)\n",
    "\n",
    "# 使用之前的标签数据\n",
    "labels_df = bug_type_df  # 或者使用 species_df 作为标签\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow_histograms, labels_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# 训练随机森林分类器\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# 预测并计算准确度\n",
    "y_pred_rf = rf_classifier.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(f\"随机森林分类准确度: {accuracy_rf}\")\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 训练支持向量机分类器\n",
    "svm_classifier = OneVsRestClassifier(SVC(kernel='linear', random_state=42))\n",
    "svm_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# 预测并计算准确度\n",
    "y_pred_svm = svm_classifier.predict(X_test_scaled)\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"支持向量机分类准确度 (One-vs-Rest): {accuracy_svm}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\features2d\\src\\sift.dispatch.cpp:515: error: (-5:Bad argument) mask has incorrect type (!=CV_8UC1) in function 'cv::SIFT_Impl::detectAndCompute'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 24\u001B[0m\n\u001B[0;32m     20\u001B[0m             \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNo descriptors found for an image.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     22\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m all_features\n\u001B[1;32m---> 24\u001B[0m sift_features \u001B[38;5;241m=\u001B[39m \u001B[43mextract_sift_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmasks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcluster\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KMeans\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# 聚类数目（可调）\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 16\u001B[0m, in \u001B[0;36mextract_sift_features\u001B[1;34m(images, masks)\u001B[0m\n\u001B[0;32m     13\u001B[0m all_features \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m img, mask \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(images, masks):\n\u001B[1;32m---> 16\u001B[0m     keypoints, descriptors \u001B[38;5;241m=\u001B[39m \u001B[43msift\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetectAndCompute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     17\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m descriptors \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     18\u001B[0m         all_features\u001B[38;5;241m.\u001B[39mappend(descriptors)\n",
      "\u001B[1;31merror\u001B[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\features2d\\src\\sift.dispatch.cpp:515: error: (-5:Bad argument) mask has incorrect type (!=CV_8UC1) in function 'cv::SIFT_Impl::detectAndCompute'\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:25.110308Z",
     "start_time": "2024-05-29T19:55:22.872562Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# CNN Model using PyTorch\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Change to long for classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # Change to long for classification\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train CNN model\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "cnn_model = CNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = cnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_accuracy = correct / total\n",
    "print(f\"CNN Accuracy: {cnn_accuracy}\")\n",
    "\n",
    "# Autoencoder using PyTorch\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# Train autoencoder\n",
    "autoencoder = Autoencoder(input_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    autoencoder.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = autoencoder(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Extract features using autoencoder\n",
    "autoencoder.eval()\n",
    "encoded_train = autoencoder.encoder(X_train_tensor).detach().numpy()\n",
    "encoded_test = autoencoder.encoder(X_test_tensor).detach().numpy()\n",
    "\n",
    "# Train classifier on encoded features\n",
    "encoded_rf_model = RandomForestClassifier(n_estimators=100)\n",
    "encoded_rf_model.fit(encoded_train, y_train)\n",
    "encoded_rf_pred = encoded_rf_model.predict(encoded_test)\n",
    "encoded_rf_accuracy = accuracy_score(y_test, encoded_rf_pred)\n",
    "print(f\"Autoencoder + Random Forest Accuracy: {encoded_rf_accuracy}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 11.334738084248134\n",
      "Epoch 2, Loss: 8.837949667658124\n",
      "Epoch 3, Loss: 4.870382274900164\n",
      "Epoch 4, Loss: 2.227934922490801\n",
      "Epoch 5, Loss: 1.6491130760737829\n",
      "Epoch 6, Loss: 0.9409172364643642\n",
      "Epoch 7, Loss: 0.6558540889195034\n",
      "Epoch 8, Loss: 0.5988060491425651\n",
      "Epoch 9, Loss: 0.5732268861361912\n",
      "Epoch 10, Loss: 0.5581392475536892\n",
      "CNN Accuracy: 0.66\n",
      "Epoch 1, Loss: 4874.539620535715\n",
      "Epoch 2, Loss: 4863.079380580357\n",
      "Epoch 3, Loss: 4823.917271205357\n",
      "Epoch 4, Loss: 4849.474051339285\n",
      "Epoch 5, Loss: 4854.509137834822\n",
      "Epoch 6, Loss: 4858.455496651785\n",
      "Epoch 7, Loss: 4855.201032366072\n",
      "Epoch 8, Loss: 4904.05810546875\n",
      "Epoch 9, Loss: 4881.471470424107\n",
      "Epoch 10, Loss: 4898.097237723215\n",
      "Autoencoder + Random Forest Accuracy: 0.52\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:25.157066Z",
     "start_time": "2024-05-29T19:55:25.113937Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class RealCNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(RealCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(input_size, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(512)\n",
    "        self.conv2 = nn.Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256, num_classes)\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.bn1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.bn2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "RealCNN_model = RealCNN(input_size, num_classes)\n",
    "RealCNN_model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(RealCNN_model.parameters(), lr=0.001)\n",
    "def train():\n",
    "    with torch.no_grad():\n",
    "        for epoch in range(10):\n",
    "            RealCNN_model.train()\n",
    "            running_loss = 0.0\n",
    "            for inputs, labels in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = RealCNN_model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                running_loss += loss.item()\n",
    "                print(running_loss)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-29T19:55:26.816083Z",
     "start_time": "2024-05-29T19:55:25.162411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "# 定义数据集类\n",
    "class BeeDataset(Dataset):\n",
    "    def __init__(self, img_dir, mask_dir, labels, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.mask_dir = mask_dir\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(labels.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, f\"{img_id}.jpg\")\n",
    "        mask_path = os.path.join(self.mask_dir, f\"binary_{img_id}.tif\")\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        label = self.labels.iloc[idx]\n",
    "        return image, label\n",
    "\n",
    "# 定义数据增强和预处理\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# 加载标签数据\n",
    "labels_df = pd.read_excel('train/classif.xlsx')\n",
    "labels_df['ID'] = labels_df.index + 1\n",
    "\n",
    "# 创建数据集和数据加载器\n",
    "train_dataset = BeeDataset(img_dir='train', mask_dir='train/masks', labels=labels_df, transform=data_transforms)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 32 * 32)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(labels_df['bug_type'].unique())\n",
    "model = SimpleCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"CNN Accuracy: {accuracy}\")\n"
   ],
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() got an unexpected keyword argument 'image'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 84\u001B[0m\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m     83\u001B[0m     running_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[1;32m---> 84\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m train_loader:\n\u001B[0;32m     85\u001B[0m         optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     86\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(images)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[1;32m~\\anaconda3\\envs\\cv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[0;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "Cell \u001B[1;32mIn[9], line 34\u001B[0m, in \u001B[0;36mBeeDataset.__getitem__\u001B[1;34m(self, idx)\u001B[0m\n\u001B[0;32m     31\u001B[0m mask \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(mask_path, cv2\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform:\n\u001B[1;32m---> 34\u001B[0m     augmented \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     35\u001B[0m     image \u001B[38;5;241m=\u001B[39m augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m     36\u001B[0m     mask \u001B[38;5;241m=\u001B[39m augmented[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmask\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mTypeError\u001B[0m: __call__() got an unexpected keyword argument 'image'"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class AugmentedDataset(Dataset):\n",
    "    def __init__(self, tensor_data, tensor_labels, transform=None):\n",
    "        self.data = tensor_data\n",
    "        self.labels = tensor_labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample, label\n",
    "\n",
    "# Define custom data augmentation transformations\n",
    "def add_noise(data, noise_factor=0.1):\n",
    "    noise = torch.randn_like(data) * noise_factor\n",
    "    return data + noise\n",
    "\n",
    "def scale_data(data, scale_factor=0.1):\n",
    "    scale = torch.randn_like(data) * scale_factor + 1.0\n",
    "    return data * scale\n",
    "\n",
    "def custom_transform(data):\n",
    "    data = add_noise(data)\n",
    "    data = scale_data(data)\n",
    "    return data\n",
    "\n",
    "# Create augmented dataset with custom transformations\n",
    "augmented_train_dataset = AugmentedDataset(X_train_tensor, y_train_tensor, transform=custom_transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(augmented_train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the CNN Model using PyTorch\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Change to long for classification\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # Change to long for classification\n",
    "\n",
    "# Initialize the CNN model\n",
    "input_size = X_train.shape[1]\n",
    "num_classes = len(np.unique(y_train))\n",
    "cnn_model = CNN(input_size, num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
    "\n",
    "# Train CNN model with augmented data\n",
    "for epoch in range(10):\n",
    "    cnn_model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = cnn_model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "# Evaluate CNN model\n",
    "cnn_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = cnn_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "cnn_accuracy = correct / total\n",
    "print(f\"CNN Accuracy after data augmentation: {cnn_accuracy}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
